{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/Z+Wlc+nt2nKAkzTXVq3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulsamadkhan/Tutorial/blob/main/Basic_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic functionality of RNN"
      ],
      "metadata": {
        "id": "35gTPqrRXlXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing the text for RNN\n",
        "The provided Python code performs a series of steps to download and preprocess a text file, create mappings for characters to their indices and vice versa, create subsequences of a specified length, batch those subsequences, and convert them into TensorFlow tensors. Here's a detailed explanation of each part of the code:\n",
        "\n",
        "1. **Importing Libraries**:\n",
        "   - `import requests`: This library is used to send HTTP requests and retrieve the text file from a given URL.\n",
        "   - `import tensorflow as tf`: TensorFlow is imported for creating and manipulating tensors, which are used for neural network training and other numerical operations.\n",
        "   - `import string`: The `string` module is imported, but it is not used in the provided code.\n",
        "\n",
        "2. **Step 1: Download and Read a Text File (`download_and_read_text_file`)**:\n",
        "   - The `download_and_read_text_file` function takes a URL as input and downloads the text file from that URL.\n",
        "   - It checks the HTTP response status code (200 for success) and converts the text to lowercase.\n",
        "   - If successful, the function returns the downloaded text.\n",
        "\n",
        "3. **Step 2: Create Character-to-Index and Index-to-Character Dictionaries (`create_char_mappings`)**:\n",
        "   - The `create_char_mappings` function takes the downloaded text as input.\n",
        "   - It extracts unique characters from the text and creates two dictionaries:\n",
        "     - `char_to_index`: Maps characters to their respective indices.\n",
        "     - `index_to_char`: Maps indices to their corresponding characters.\n",
        "   - These dictionaries are essential for encoding and decoding text during text processing and modeling.\n",
        "\n",
        "4. **Step 3: Create Subsequences of a Specified Length (`create_subsequences`)**:\n",
        "   - The `create_subsequences` function takes the downloaded text and a specified sequence length as input.\n",
        "   - It generates subsequences of characters by sliding a window of the given length over the text.\n",
        "   - For each subsequence, it creates an input sequence and a target sequence, where the target sequence is one character ahead of the input sequence.\n",
        "   - These subsequences are used for training sequence models like RNNs.\n",
        "\n",
        "5. **Step 4: Batch the Subsequences (`batch_sequences`)**:\n",
        "   - The `batch_sequences` function takes a list of subsequences and a batch size as input.\n",
        "   - It groups the subsequences into batches of the specified size.\n",
        "\n",
        "6. **Step 5: Create Input and Output Sequences and Convert to Tensors (`create_input_output_tensors` and `create_tensor_dataset`)**:\n",
        "   - The `create_input_output_tensors` function takes a subsequence and the character-to-index dictionary as input.\n",
        "   - It converts input and target subsequences into tensors with characters mapped to their indices.\n",
        "   - The `create_tensor_dataset` function processes multiple subsequences, converting them into TensorFlow tensors.\n",
        "\n",
        "7. **Downloading a File with URL**:\n",
        "   - A file URL (`file_url`) is provided as an example, and the `download_and_read_text_file` function is called to download and read the text from that URL.\n",
        "   - You should replace the example URL with the actual URL of the text file you want to process.\n",
        "\n",
        "This code snippet sets up the initial steps for processing and preparing text data for further analysis or modeling, such as training a character-level language model. The downloaded text is converted to lowercase, and character-to-index mappings are created for encoding and decoding the text during model training. Additionally, subsequences are generated, batched, and converted into TensorFlow tensors, which can be useful for training neural network models."
      ],
      "metadata": {
        "id": "Hux39JSeXI3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tensorflow as tf\n",
        "import string\n",
        "\n",
        "# Step 1: Download and read a text file\n",
        "def download_and_read_text_file(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        text = response.text\n",
        "        return text.lower()\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download the file from {url}\")\n",
        "\n",
        "# Step 2: Create a character-to-index and index-to-character dictionary\n",
        "def create_char_mappings(text):\n",
        "    unique_chars = list(set(text))\n",
        "    char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "    index_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
        "    return char_to_index, index_to_char\n",
        "\n",
        "# Step 3: Create subsequences of a specified length\n",
        "def create_subsequences(text, sequence_length):\n",
        "    sequences = []\n",
        "    for i in range(0, len(text) - sequence_length, sequence_length):\n",
        "        input_seq = text[i:i + sequence_length]\n",
        "        target_seq = text[i + 1:i + sequence_length + 1]\n",
        "        sequences.append((input_seq, target_seq))\n",
        "    return sequences\n",
        "\n",
        "# Step 4: Batch the subsequences\n",
        "def batch_sequences(sequences, batch_size):\n",
        "    batches = []\n",
        "    for i in range(0, len(sequences), batch_size):\n",
        "        batch = sequences[i:i + batch_size]\n",
        "        batches.append(batch)\n",
        "    return batches\n",
        "\n",
        "# Step 5: Create input and output sequences\n",
        "def create_input_output_tensors(sequence, char_to_index):\n",
        "    input_sequence, target_sequence = sequence\n",
        "    input_tensor = [char_to_index[char] for char in input_sequence]\n",
        "    target_tensor = [char_to_index[char] for char in target_sequence]\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "# Step 5: Convert subsequences to TensorFlow tensors\n",
        "def create_tensor_dataset(sequences, char_to_index):\n",
        "    input_tensors = []\n",
        "    target_tensors = []\n",
        "    for sequence in sequences:\n",
        "        input_seq, target_seq = sequence\n",
        "        input_tensor = tf.convert_to_tensor([char_to_index[char] for char in input_seq], dtype=tf.int32)\n",
        "        target_tensor = tf.convert_to_tensor([char_to_index[char] for char in target_seq], dtype=tf.int32)\n",
        "        input_tensors.append(input_tensor)\n",
        "        target_tensors.append(target_tensor)\n",
        "    return input_tensors, target_tensors\n",
        "\n",
        "# downloading a file with URL\n",
        "file_url = 'https://example-files.online-convert.com/document/txt/example.txt'  # Replace with the actual URL\n",
        "text = download_and_read_text_file(file_url)\n",
        "\n"
      ],
      "metadata": {
        "id": "rzpcrtnykkBJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index, index_to_char = create_char_mappings(text)\n",
        "char_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkftpk-ejDzF",
        "outputId": "6e169b19-21c4-46ad-dba5-5a567c011f1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'z': 0,\n",
              " 'c': 1,\n",
              " 'i': 2,\n",
              " 'b': 3,\n",
              " ' ': 4,\n",
              " 'm': 5,\n",
              " '-': 6,\n",
              " '2': 7,\n",
              " '.': 8,\n",
              " 'v': 9,\n",
              " 'a': 10,\n",
              " 'u': 11,\n",
              " '/': 12,\n",
              " ',': 13,\n",
              " '#': 14,\n",
              " 'h': 15,\n",
              " ')': 16,\n",
              " 'p': 17,\n",
              " 'w': 18,\n",
              " '1': 19,\n",
              " 'x': 20,\n",
              " 'n': 21,\n",
              " 'g': 22,\n",
              " 'q': 23,\n",
              " 'l': 24,\n",
              " '\\n': 25,\n",
              " 'y': 26,\n",
              " '\"': 27,\n",
              " '(': 28,\n",
              " '4': 29,\n",
              " 't': 30,\n",
              " 'e': 31,\n",
              " '0': 32,\n",
              " 'k': 33,\n",
              " 's': 34,\n",
              " 'r': 35,\n",
              " 'j': 36,\n",
              " ':': 37,\n",
              " ';': 38,\n",
              " 'f': 39,\n",
              " 'o': 40,\n",
              " 'd': 41,\n",
              " '_': 42}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 20\n",
        "subsequences = create_subsequences(text, sequence_length)\n",
        "subsequences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNQ95noijYWn",
        "outputId": "c7214b7c-b9d4-4bba-a7ca-b363cb5e080a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('txt test file\\npurpos', 'xt test file\\npurpose'),\n",
              " ('e: provide example o', ': provide example of'),\n",
              " ('f this file type\\ndoc', ' this file type\\ndocu'),\n",
              " ('ument file type: txt', 'ment file type: txt\\n'),\n",
              " ('\\nversion: 1.0\\nremark', 'version: 1.0\\nremark:'),\n",
              " (':\\n\\nexample content:\\n', '\\n\\nexample content:\\nt'),\n",
              " ('the names \"john doe\"', 'he names \"john doe\" '),\n",
              " (' for males, \"jane do', 'for males, \"jane doe'),\n",
              " ('e\" or \"jane roe\" for', '\" or \"jane roe\" for '),\n",
              " (' females, or \"jonnie', 'females, or \"jonnie '),\n",
              " (' doe\" and \"janie doe', 'doe\" and \"janie doe\"'),\n",
              " ('\" for children, or j', ' for children, or ju'),\n",
              " ('ust \"doe\" non-gender', 'st \"doe\" non-gender-'),\n",
              " ('-specifically are us', 'specifically are use'),\n",
              " ('ed as placeholder na', 'd as placeholder nam'),\n",
              " ('mes for a party whos', 'es for a party whose'),\n",
              " ('e true identity is u', ' true identity is un'),\n",
              " ('nknown or must be wi', 'known or must be wit'),\n",
              " ('thheld in a legal ac', 'hheld in a legal act'),\n",
              " ('tion, case, or discu', 'ion, case, or discus'),\n",
              " ('ssion. the names are', 'sion. the names are '),\n",
              " (' also used to refer ', 'also used to refer t'),\n",
              " ('to acorpse or hospit', 'o acorpse or hospita'),\n",
              " ('al patient whose ide', 'l patient whose iden'),\n",
              " ('ntity is unknown. th', 'tity is unknown. thi'),\n",
              " ('is practice is widel', 's practice is widely'),\n",
              " ('y used in the united', ' used in the united '),\n",
              " (' states and canada, ', 'states and canada, b'),\n",
              " ('but is rarely used i', 'ut is rarely used in'),\n",
              " ('n other english-spea', ' other english-speak'),\n",
              " ('king countries inclu', 'ing countries includ'),\n",
              " ('ding the united king', 'ing the united kingd'),\n",
              " ('dom itself, from whe', 'om itself, from wher'),\n",
              " ('re the use of \"john ', 'e the use of \"john d'),\n",
              " ('doe\" in a legal cont', 'oe\" in a legal conte'),\n",
              " ('ext originates. the ', 'xt originates. the n'),\n",
              " ('names joe bloggs or ', 'ames joe bloggs or j'),\n",
              " ('john smith are used ', 'ohn smith are used i'),\n",
              " ('in the uk instead, a', 'n the uk instead, as'),\n",
              " ('s well as in austral', ' well as in australi'),\n",
              " ('ia and new zealand.\\n', 'a and new zealand.\\n\\n'),\n",
              " ('\\njohn doe is sometim', 'john doe is sometime'),\n",
              " ('es used to refer to ', 's used to refer to a'),\n",
              " ('a typical male in ot', ' typical male in oth'),\n",
              " ('her contexts as well', 'er contexts as well,'),\n",
              " (', in a similar manne', ' in a similar manner'),\n",
              " ('r to john q. public,', ' to john q. public, '),\n",
              " (' known in great brit', 'known in great brita'),\n",
              " ('ain as joe public, j', 'in as joe public, jo'),\n",
              " ('ohn smith or joe blo', 'hn smith or joe blog'),\n",
              " ('ggs. for example, th', 'gs. for example, the'),\n",
              " ('e first name listed ', ' first name listed o'),\n",
              " ('on a form is often j', 'n a form is often jo'),\n",
              " ('ohn doe, along with ', 'hn doe, along with a'),\n",
              " ('a fictional address ', ' fictional address o'),\n",
              " ('or other fictional i', 'r other fictional in'),\n",
              " ('nformation to provid', 'formation to provide'),\n",
              " ('e an example of how ', ' an example of how t'),\n",
              " ('to fill in the form.', 'o fill in the form. '),\n",
              " (' the name is also us', 'the name is also use'),\n",
              " ('ed frequently in pop', 'd frequently in popu'),\n",
              " ('ular culture, for ex', 'lar culture, for exa'),\n",
              " ('ample in the frank c', 'mple in the frank ca'),\n",
              " ('apra film meet john ', 'pra film meet john d'),\n",
              " ('doe. john doe was al', 'oe. john doe was als'),\n",
              " ('so the name of a 200', 'o the name of a 2002'),\n",
              " ('2 american televisio', ' american television'),\n",
              " ('n series.\\n\\nsimilarly', ' series.\\n\\nsimilarly,'),\n",
              " (', a child or baby wh', ' a child or baby who'),\n",
              " ('ose identity is unkn', 'se identity is unkno'),\n",
              " ('own may be referred ', 'wn may be referred t'),\n",
              " ('to as baby doe. a no', 'o as baby doe. a not'),\n",
              " ('torious murder case ', 'orious murder case i'),\n",
              " ('in kansas city, miss', 'n kansas city, misso'),\n",
              " ('ouri, referred to th', 'uri, referred to the'),\n",
              " ('e baby victim as pre', ' baby victim as prec'),\n",
              " ('cious doe. other uni', 'ious doe. other unid'),\n",
              " ('dentified female mur', 'entified female murd'),\n",
              " ('der victims are cali', 'er victims are cali '),\n",
              " (' doe and princess do', 'doe and princess doe'),\n",
              " ('e. additional person', '. additional persons'),\n",
              " ('s may be called jame', ' may be called james'),\n",
              " ('s doe, judy doe, etc', ' doe, judy doe, etc.'),\n",
              " ('. however, to avoid ', ' however, to avoid p'),\n",
              " ('possible confusion, ', 'ossible confusion, i'),\n",
              " ('if two anonymous or ', 'f two anonymous or u'),\n",
              " ('unknown parties are ', 'nknown parties are c'),\n",
              " ('cited in a specific ', 'ited in a specific c'),\n",
              " ('case or action, the ', 'ase or action, the s'),\n",
              " ('surnames doe and roe', 'urnames doe and roe '),\n",
              " (' may be used simulta', 'may be used simultan'),\n",
              " ('neously; for example', 'eously; for example,'),\n",
              " (', \"john doe v. jane ', ' \"john doe v. jane r'),\n",
              " ('roe\". if several ano', 'oe\". if several anon'),\n",
              " ('nymous parties are r', 'ymous parties are re'),\n",
              " ('eferenced, they may ', 'ferenced, they may s'),\n",
              " ('simply be labelled j', 'imply be labelled jo'),\n",
              " ('ohn doe #1, john doe', 'hn doe #1, john doe '),\n",
              " (' #2, etc. (the u.s. ', '#2, etc. (the u.s. o'),\n",
              " ('operation delego cit', 'peration delego cite'),\n",
              " ('ed 21 (numbered) \"jo', 'd 21 (numbered) \"joh'),\n",
              " ('hn doe\"s) or labelle', 'n doe\"s) or labelled'),\n",
              " ('d with other variant', ' with other variants'),\n",
              " ('s of doe / roe / poe', ' of doe / roe / poe '),\n",
              " (' / etc. other early ', '/ etc. other early a'),\n",
              " ('alternatives such as', 'lternatives such as '),\n",
              " (' john stiles and ric', 'john stiles and rich'),\n",
              " ('hard miles are now r', 'ard miles are now ra'),\n",
              " ('arely used, and mary', 'rely used, and mary '),\n",
              " (' major has been used', 'major has been used '),\n",
              " (' in some american fe', 'in some american fed'),\n",
              " ('deral cases.\\n\\n\\n\\nfile', 'eral cases.\\n\\n\\n\\nfile '),\n",
              " (' created by https://', 'created by https://w'),\n",
              " ('www.online-convert.c', 'ww.online-convert.co'),\n",
              " ('om\\nmore example file', 'm\\nmore example files'),\n",
              " ('s: https://www.onlin', ': https://www.online'),\n",
              " ('e-convert.com/file-t', '-convert.com/file-ty'),\n",
              " ('ype\\ntext of example ', 'pe\\ntext of example c'),\n",
              " ('content: wikipedia (', 'ontent: wikipedia (h'),\n",
              " ('https://en.wikipedia', 'ttps://en.wikipedia.'),\n",
              " ('.org/wiki/john_doe)\\n', 'org/wiki/john_doe)\\nl'),\n",
              " ('license: attribution', 'icense: attribution-'),\n",
              " ('-sharealike 4.0 (htt', 'sharealike 4.0 (http'),\n",
              " ('ps://creativecommons', 's://creativecommons.'),\n",
              " ('.org/licenses/by-sa/', 'org/licenses/by-sa/4'),\n",
              " ('4.0/)\\n\\nfeel free to ', '.0/)\\n\\nfeel free to u'),\n",
              " ('use and share the fi', 'se and share the fil'),\n",
              " ('le according to the ', 'e according to the l')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "072sxpiHe2Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "batches = batch_sequences(subsequences, batch_size)\n",
        "input, target=batches[0][0]\n",
        "#print(input)\n",
        "#print(\"-----\")\n",
        "#print(target)\n",
        "input_seq, target_seq = batches[0][0]\n",
        "input_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PLxsRafyo4oH",
        "outputId": "1f470167-1d9c-4781-ef35-49c9daf966b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'txt test file\\npurpos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor, target_tensor = create_input_output_tensors((input_seq, target_seq), char_to_index)\n",
        "len(input_tensor)"
      ],
      "metadata": {
        "id": "brNsQpaof0-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0923403c-d104-400a-f8c1-834ca9f0eaa1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make the whole dataste as tensor\n",
        "input_tensors, target_tensors = create_tensor_dataset(subsequences, char_to_index)\n",
        "input_tensors[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kXK-50gmcHc",
        "outputId": "352bfa30-9812-4cbc-b960-0fade17efdac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([20])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "# RNN model\n",
        "Certainly! The provided Python code defines a recurrent neural network (RNN) model using TensorFlow's Keras API. This code snippet includes model architecture definition, hyperparameter settings, model creation, and compilation. Here's a detailed explanation of each part of the code:\n",
        "\n",
        "1. **Define the RNN Model (`SimpleRNNModel`)**:\n",
        "   - The code defines a custom RNN model named `SimpleRNNModel` that inherits from `tf.keras.Model`.\n",
        "   - In the constructor (`__init__` method), the following layers are defined:\n",
        "     - `embedding`: An embedding layer that maps input tokens to dense vectors. The size of the vocabulary (`vocab_size`) and the embedding dimension (`embedding_dim`) are provided as parameters.\n",
        "     - `rnn`: A SimpleRNN layer with `rnn_units` units. It returns sequences (`return_sequences=True`) to provide sequences of outputs at each time step. The `recurrent_initializer` is set to 'glorot_uniform' for weight initialization.\n",
        "     - `dense`: A dense layer with `vocab_size` units, which is used to produce the final output.\n",
        "\n",
        "2. **Hyperparameters**:\n",
        "   - Hyperparameters for the model are defined:\n",
        "     - `vocab_size`: The size of the vocabulary. It's set based on the length of a character-to-index mapping (not shown in the provided code).\n",
        "     - `embedding_dim`: The dimension of the word embeddings.\n",
        "     - `rnn_units`: The number of units (neurons) in the RNN layer.\n",
        "     - `sequence_length`: The length of input sequences. You can adjust this value according to your dataset.\n",
        "\n",
        "3. **Create the Model**:\n",
        "   - The RNN model is created by instantiating the `SimpleRNNModel` class with the specified hyperparameters. The resulting model is stored in the `model` variable.\n",
        "\n",
        "4. **Compile the Model**:\n",
        "   - The model is compiled to prepare it for training. The following settings are specified during compilation:\n",
        "     - `optimizer='adam'`: The Adam optimizer is used for gradient descent optimization.\n",
        "     - `loss='sparse_categorical_crossentropy'`: This is a common choice for text classification tasks where the target values are integers representing class labels.\n",
        "\n",
        "This code sets up a custom RNN model for text processing tasks. You can use this model for tasks like text generation, sentiment analysis, or any NLP-related problem where sequence data is involved. Before training the model, you would typically need to prepare your data, tokenize it, and convert it to a format that the model can accept. Additionally, you may need to define training loops, preprocess data, and fit the model to your specific dataset."
      ],
      "metadata": {
        "id": "oCa7nOQ9SuEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model with one hidden layer\n",
        "class SimpleRNNModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super(SimpleRNNModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform')\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.rnn(x)\n",
        "        output = self.dense(x)\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(char_to_index)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "sequence_length = 50  # You can adjust this based on your dataset\n",
        "\n",
        "# Create the model\n",
        "model = SimpleRNNModel(vocab_size, embedding_dim, rnn_units)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "DHQOtK0LnaGb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Training RNN Loop\n",
        "\n",
        "1. **`train_rnn_model` Function**:\n",
        "   - This function is designed to train an RNN model. It takes the following parameters:\n",
        "     - `input_tensors`: List of input tensors (sequences of input data).\n",
        "     - `target_tensors`: List of target tensors (sequences of target data).\n",
        "     - `epochs` (optional, default: 10): Number of training epochs (complete passes through the dataset).\n",
        "     - `batch_size` (optional, default: 32): Mini-batch size for training.\n",
        "\n",
        "2. **Training Loop**:\n",
        "   - The function starts with a loop over the specified number of `epochs`. During each epoch, the entire dataset is processed in mini-batches.\n",
        "\n",
        "3. **Mini-Batch Loop**:\n",
        "   - Within each epoch, there is another loop that iterates over mini-batches of data.\n",
        "   - The loop variable `i` is used to keep track of the starting index of each mini-batch.\n",
        "\n",
        "4. **Print Current Batch Index**:\n",
        "   - For monitoring and debugging purposes, the code prints the current batch index (`i`) to the console. This helps you keep track of the progress during training.\n",
        "\n",
        "5. **Extract Mini-Batch Data**:\n",
        "   - Inside the mini-batch loop, the code extracts a mini-batch of input and target tensors.\n",
        "   - `x` contains a stack of input tensors, and `y` contains a stack of corresponding target tensors.\n",
        "\n",
        "6. **Model Training (Fitting)**:\n",
        "   - The mini-batch of input (`x`) and target (`y`) tensors is used to train (fit) the RNN model using the `model.fit` method. The `batch_size` and `verbose` parameters are specified during training.\n",
        "   - `batch_size` determines the size of the mini-batch used for gradient updates.\n",
        "   - `verbose=2` means that training progress is printed to the console for each epoch, including loss and other metrics.\n",
        "\n",
        "This code essentially sets up a training loop for an RNN model, enabling you to train the model on sequential data stored in input and target tensors. The training process involves iterating over the dataset multiple times (controlled by `epochs`) and updating the model's weights to minimize the specified loss function. The model used for training should be defined and compiled separately."
      ],
      "metadata": {
        "id": "6JMkQjZ8aVKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn_model(input_tensors, target_tensors, epochs=10, batch_size=32):\n",
        "  for epoch in range(epochs):\n",
        "    for i in range(0, len(input_tensors) - batch_size, batch_size):\n",
        "      print(i)\n",
        "      x = tf.stack(input_tensors[i:i + batch_size], axis=0)\n",
        "      y = tf.stack(target_tensors[i:i + batch_size], axis=0)\n",
        "      model.fit(x, y, batch_size=batch_size, verbose=2)"
      ],
      "metadata": {
        "id": "rJzjqc4RQjab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rnn_model(input_tensors, target_tensors, epochs=1, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMtJxGxbTmmq",
        "outputId": "ec6138ea-0788-4b7b-dfb2-a8b5f61c9942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 - 0s - loss: 5.7145 - 38ms/epoch - 38ms/step\n",
            "10\n",
            "1/1 - 0s - loss: 4.7206 - 53ms/epoch - 53ms/step\n",
            "20\n",
            "1/1 - 0s - loss: 4.3070 - 41ms/epoch - 41ms/step\n",
            "30\n",
            "1/1 - 0s - loss: 3.9285 - 41ms/epoch - 41ms/step\n",
            "40\n",
            "1/1 - 0s - loss: 4.0939 - 44ms/epoch - 44ms/step\n",
            "50\n",
            "1/1 - 0s - loss: 3.8180 - 52ms/epoch - 52ms/step\n",
            "60\n",
            "1/1 - 0s - loss: 4.7247 - 47ms/epoch - 47ms/step\n",
            "70\n",
            "1/1 - 0s - loss: 3.8631 - 38ms/epoch - 38ms/step\n",
            "80\n",
            "1/1 - 0s - loss: 4.0075 - 49ms/epoch - 49ms/step\n",
            "90\n",
            "1/1 - 0s - loss: 4.8541 - 41ms/epoch - 41ms/step\n",
            "100\n",
            "1/1 - 0s - loss: 4.4273 - 44ms/epoch - 44ms/step\n",
            "110\n",
            "1/1 - 0s - loss: 7.1211 - 40ms/epoch - 40ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhSRSTuN_u70",
        "outputId": "31c625b3-a561-462d-e091-a47d58d81fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2': 0,\n",
              " 't': 1,\n",
              " 'n': 2,\n",
              " 'r': 3,\n",
              " '_': 4,\n",
              " 'l': 5,\n",
              " '/': 6,\n",
              " ':': 7,\n",
              " '.': 8,\n",
              " 'g': 9,\n",
              " 'h': 10,\n",
              " 'k': 11,\n",
              " '-': 12,\n",
              " '\"': 13,\n",
              " 'd': 14,\n",
              " 'p': 15,\n",
              " 'v': 16,\n",
              " 's': 17,\n",
              " '0': 18,\n",
              " 'x': 19,\n",
              " 'w': 20,\n",
              " ' ': 21,\n",
              " 'z': 22,\n",
              " 'f': 23,\n",
              " 'j': 24,\n",
              " ';': 25,\n",
              " 'a': 26,\n",
              " ')': 27,\n",
              " 'y': 28,\n",
              " '\\n': 29,\n",
              " 'q': 30,\n",
              " '#': 31,\n",
              " 'i': 32,\n",
              " 'e': 33,\n",
              " 'c': 34,\n",
              " '(': 35,\n",
              " 'o': 36,\n",
              " '4': 37,\n",
              " ',': 38,\n",
              " 'b': 39,\n",
              " '1': 40,\n",
              " 'u': 41,\n",
              " 'm': 42}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating the text from RNN Model\n",
        "\n",
        "1. **The function `generate_text`** is designed to generate text using a trained model. It takes the following parameters:\n",
        "   - `model`: The trained neural network model used for text generation.\n",
        "   - `start_string`: The initial text that serves as a seed for text generation.\n",
        "   - `num_generate`: The number of characters to generate beyond the start string.\n",
        "\n",
        "2. The code converts the `start_string` to lowercase and initializes some variables for text generation.\n",
        "\n",
        "3. It iteratively generates characters:\n",
        "   - The function loops 5 times to generate 5 characters. You can adjust this number as needed.\n",
        "   - The model is used to make predictions based on the input evaluation (`input_eval`).\n",
        "\n",
        "4. In each iteration, the following steps occur:\n",
        "   - The shape of the predictions from the model is printed.\n",
        "   - The predictions are squeezed to remove the batch dimension for easier handling.\n",
        "   - A character ID is sampled from the predictions using random sampling.\n",
        "   - The ID of the predicted character is printed.\n",
        "   - The predicted character's ID is used to prepare the next input for text generation.\n",
        "\n",
        "5. The generated characters are appended to the `text_generated` list.\n",
        "\n",
        "6. Finally, the generated text is returned by concatenating the `start_string` with the characters generated in the loop.\n",
        "\n",
        "7. The generated text is printed to the console.\n",
        "\n",
        "This code is used for generating text sequences based on a trained language model, where the model takes a seed text and predicts the next characters in the sequence, making it useful for various natural language generation tasks."
      ],
      "metadata": {
        "id": "rdeiEce3b4gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the trained model\n",
        "def generate_text(model, start_string,  num_generate):\n",
        "  start_string = start_string.lower()\n",
        "  default_index = -1\n",
        "  input_eval = [char_to_index.get(s,-1) for s in start_string]\n",
        "  print(input_eval)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(input_eval)\n",
        "  text_generated = []\n",
        "\n",
        "  for _ in range(5):\n",
        "    predictions = model(input_eval)\n",
        "    print(predictions.shape)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    print(predictions.shape)\n",
        "\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "    print(predicted_id)\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    print(input_eval)\n",
        "    text_generated.append(index_to_char[predicted_id])\n",
        "  return start_string + ''.join(text_generated)\n",
        "\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, start_string=\"The quick brown\", num_generate=5)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6hg-mMek6vh",
        "outputId": "6c089bea-0a9c-47b0-e16a-4ecf6d336c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 10, 33, 21, 30, 41, 32, 34, 11, 21, 39, 3, 36, 20, 2]\n",
            "tf.Tensor([[ 1 10 33 21 30 41 32 34 11 21 39  3 36 20  2]], shape=(1, 15), dtype=int32)\n",
            "(1, 15, 43)\n",
            "(15, 43)\n",
            "24\n",
            "tf.Tensor([[24]], shape=(1, 1), dtype=int32)\n",
            "(1, 1, 43)\n",
            "(1, 43)\n",
            "13\n",
            "tf.Tensor([[13]], shape=(1, 1), dtype=int32)\n",
            "(1, 1, 43)\n",
            "(1, 43)\n",
            "0\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "(1, 1, 43)\n",
            "(1, 43)\n",
            "30\n",
            "tf.Tensor([[30]], shape=(1, 1), dtype=int32)\n",
            "(1, 1, 43)\n",
            "(1, 43)\n",
            "29\n",
            "tf.Tensor([[29]], shape=(1, 1), dtype=int32)\n",
            "the quick brownj\"2q\n",
            "\n"
          ]
        }
      ]
    }
  ]
}